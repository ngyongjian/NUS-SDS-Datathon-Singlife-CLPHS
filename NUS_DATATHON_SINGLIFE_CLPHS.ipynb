{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ph8MiO8Merq"
      },
      "source": [
        "##### The cell below is for you to keep track of the libraries used and install those libraries quickly\n",
        "##### Ensure that the proper library names are used and the syntax of `%pip install PACKAGE_NAME` is followed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JpQpWDKMerr"
      },
      "outputs": [],
      "source": [
        "#%pip install pandas\n",
        "#%pip install matplotlib\n",
        "# add commented pip installation lines for packages used as shown above for ease of testing\n",
        "# the line should be of the format %pip install PACKAGE_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qP39e8UMers"
      },
      "source": [
        "## **DO NOT CHANGE** the filepath variable\n",
        "##### Instead, create a folder named 'data' in your current working directory and\n",
        "##### have the .parquet file inside that. A relative path *must* be used when loading data into pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnY8Eq3xMers"
      },
      "outputs": [],
      "source": [
        "# Can have as many cells as you want for code\n",
        "import pandas as pd\n",
        "filepath = \"./data/catB_train.parquet\"\n",
        "# the initialised filepath MUST be a relative path to a folder named data that contains the parquet file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKp_4AC1Mers"
      },
      "source": [
        "### **ALL** Code for machine learning and dataset analysis should be entered below.\n",
        "##### Ensure that your code is clear and readable.\n",
        "##### Comments and Markdown notes are advised to direct attention to pieces of code you deem useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "Jnlmsb01NXqq",
        "outputId": "fe0254fc-3455-44a1-edf8-1730050ff44b"
      },
      "outputs": [
        {
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-31b995ce75f2>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display.max_columns'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;31m# Specify the path to your file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/data/catB_train.parquet'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    127\u001b[0m   )\n\u001b[1;32m    128\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "# import from drive\n",
        "from google.colab import drive\n",
        "import pyarrow.parquet as pq\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, f1_score, precision_score, recall_score\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "# Specify the path to your file\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/data/catB_train.parquet'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0llQdRaONPIs"
      },
      "outputs": [],
      "source": [
        "# import dataset\n",
        "df = pq.read_table(file_path).to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zLdkQCQwCVE"
      },
      "source": [
        "#### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEFBXAVvwv7H"
      },
      "source": [
        "First and foremost, we would like to pay attention to the target variable for our data analysis, which is `f_purchase_lh` in our case. For this target, something that we noted is that the target have only previous purchases recorded, and for non-purchasers the value is empty. This is something to note in our upcoming data preprocessing step.\n",
        "\n",
        "On the other hand, the `f_purchase_lh` column is also noted to be an imbalanced columns, with a significant majority of non-purchasers. Therefore, we could frame the problem statement to be a binary classification in imbalanced data. This is important to note for our next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cqv0tqhiv_cR"
      },
      "outputs": [],
      "source": [
        "# target var - f_purchase_lh\n",
        "df.f_purchase_lh.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aex-EvPJxi_r"
      },
      "source": [
        "##### General Client Information\n",
        "\n",
        "• clntnum: Unique identifier for the client.\n",
        "\n",
        "As the identifier, clntnum has all unique values. It should not be seen as a feature in the final model.\n",
        "\n",
        "• race_desc: Description of the client's race.\n",
        "\n",
        "Based on our observation, the race of the client are mainly Chinese, Malay and Indian. However, there are up to 3996 rows of null values, which is worth noticing and paying attention to.\n",
        "\n",
        "• ctrycode_desc: Country code indicating the client's location.\n",
        "\n",
        "The country column stores the location from which the client is from. Based on our observations, majority of the clients are from Singapore, while 20 null values are spotted.\n",
        "\n",
        "• clttype: Customer status.\n",
        "\n",
        "Based on our observations, the customer status can be split into 3 major groups, namely `p`, `g`, and `c`. Majority of the customers are from `p` status, and there is no null value for the column.\n",
        "\n",
        "• stat_flag: Flag indicating ACTIVE, LAPSED or MATURED\n",
        "\n",
        "Majority of the clients are active clients of insurance policy, and there is no null value for the column.\n",
        "\n",
        "• min_occ_date: Date of the client's first interaction or policy purchase with the company.\n",
        "\n",
        "• cltdob_fix: Fixed or corrected date of birth of the client.\n",
        "\n",
        "For min_occ_date and cltdob_fix, we do notice that there are `None` values spotted in the columns. After further researching, there are 20 and 7 `None` values in the columns respectively. This may be a sign of erroneous data.\n",
        "\n",
        "• cltsex_fix: Fixed or corrected gender of the client.\n",
        "\n",
        "For the gender distribution, it is quite balanced. However, there are 23 null values that we may want to pay attention to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8DlD4PmAxjMH"
      },
      "outputs": [],
      "source": [
        "# General Client Information\n",
        "\n",
        "# clntnum - unique identifier\n",
        "print(f'Check for uniqueness : {len(df.clntnum.unique())==len(df)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hjYmH6Fb03tS"
      },
      "outputs": [],
      "source": [
        "# race_desc - race distribution of clients\n",
        "print(f'Number of NA values : {df.race_desc.isna().sum()}')\n",
        "race = df.race_desc.value_counts().reset_index()\n",
        "plt.figure(figsize=(8, 6))\n",
        "ax = sns.barplot(x='index', y='race_desc', data=race)\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
        "plt.title('Client Race Distribution')\n",
        "plt.xlabel('')\n",
        "plt.ylabel('')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ao-5Ab1n05W3"
      },
      "outputs": [],
      "source": [
        "# ctrycode_desc - client nationality distribution\n",
        "print(f'Number of NA values : {df.ctrycode_desc.isna().sum()}')\n",
        "country = df.ctrycode_desc.value_counts().reset_index()\n",
        "plt.figure(figsize=(20, 8))\n",
        "ax = sns.barplot(x='index', y='ctrycode_desc', data=country)\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
        "plt.title('Client Countries Distribution')\n",
        "plt.xlabel('')\n",
        "plt.ylabel('')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OUI4k33r065x"
      },
      "outputs": [],
      "source": [
        "# clttype - client type\n",
        "print(f'Number of NA values : {df.clttype.isna().sum()}')\n",
        "type = df.clttype.value_counts().reset_index()\n",
        "plt.figure(figsize=(8, 6))\n",
        "ax = sns.barplot(x='index', y='clttype', data=type)\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
        "plt.title('Client Status Distribution')\n",
        "plt.xlabel('')\n",
        "plt.ylabel('')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "U70f1ixF0-vW"
      },
      "outputs": [],
      "source": [
        "# stat_flag - client status\n",
        "print(f'Number of NA values : {df.stat_flag.isna().sum()}')\n",
        "stat_flag = df.stat_flag.value_counts().reset_index()\n",
        "plt.figure(figsize=(8, 6))\n",
        "ax = sns.barplot(x='index', y='stat_flag', data=stat_flag)\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
        "plt.title('Client Insurance Flag Distribution')\n",
        "plt.xlabel('')\n",
        "plt.ylabel('')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "N9s45CMZB6fc"
      },
      "outputs": [],
      "source": [
        "# cltsex_fix - client gender\n",
        "print(f'Number of NA values : {df.cltsex_fix.isna().sum()}')\n",
        "cltsex_fix = df.cltsex_fix.value_counts().reset_index()\n",
        "plt.figure(figsize=(8, 6))\n",
        "ax = sns.barplot(x='index', y='cltsex_fix', data=cltsex_fix)\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
        "plt.title('Client Gender Distribution')\n",
        "plt.xlabel('')\n",
        "plt.ylabel('')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SfPmXZJFCNdn"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(f'Erroneous data {df.min_occ_date.unique().max()} is noticed in the min_ooc_date column')\n",
        "print(f'Erroneous data {df.cltdob_fix.unique().max()} is noticed in the cltdob_fix column')\n",
        "print(f'The earliest data is {df.min_occ_date.unique().min()} in the min_ooc_date column')\n",
        "print(f'The earliest data is {df.cltdob_fix.unique().min()} is noticed in the cltdob_fix column')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62BVoqdLOyeC"
      },
      "source": [
        "##### Client Risk and Status Indicators\n",
        "\n",
        "• flg_substandard: Flag for substandard risk clients.\n",
        "\n",
        "• flg_is_borderline_standard: Flag for borderline standard risk clients.\n",
        "\n",
        "• flg_is_revised_term: Flag if customer ever has revised terms.\n",
        "\n",
        "• flg_is_rental_flat: Indicates if the client lives in a rental flat.\n",
        "\n",
        "• flg_has_health_claim: Flag for clients with health insurance claims.\n",
        "\n",
        "• flg_has_life_claim: Flag for clients with life insurance claims.\n",
        "\n",
        "• flg_gi_claim: Flag for general insurance claims.\n",
        "\n",
        "• flg_is_proposal: Indicates if there is a policy in proposal for client.\n",
        "\n",
        "• flg_with_preauthorisation: Flag for clients with preauthorized transactions or policies.\n",
        "\n",
        "• flg_is_returned_mail: Flag for returned mail instances.\n",
        "\n",
        "For all the flags for certain conditions, they all have 1014 null values. Upon further checking, these NA values are from the same rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Oh5qZvD5TfwQ"
      },
      "outputs": [],
      "source": [
        "def show_dist(col):\n",
        "  val = df[f'{col}'].value_counts().reset_index()[f'{col}']\n",
        "  NA = df[f'{col}'].isna().sum()\n",
        "  zero = val[0]\n",
        "  if len(val)>1:\n",
        "    one = val[1]\n",
        "  else:\n",
        "    one = 0\n",
        "  return zero, one, NA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hWJxnh2ZQQa5"
      },
      "outputs": [],
      "source": [
        "lsOfCols = ['flg_substandard', 'flg_is_borderline_standard', 'flg_is_revised_term', 'flg_is_rental_flat', 'flg_has_health_claim', 'flg_has_life_claim', 'flg_gi_claim', 'flg_is_proposal', 'flg_with_preauthorisation', 'flg_is_returned_mail']\n",
        "flagDist = pd.DataFrame(columns=['Zero', 'One', 'NA value'])\n",
        "for col in lsOfCols:\n",
        "  zero, one, NA = show_dist(col)\n",
        "  new_row = {'Zero': zero, 'One': one, 'NA value': NA}\n",
        "  flagDist = pd.concat([flagDist, pd.DataFrame(new_row, index=[col])])\n",
        "flagDist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMi_OA92Tjla"
      },
      "source": [
        "##### Client Consent and Communication Preferences, Demographic and Household Information\n",
        "\n",
        "• is_consent_to_mail, is_consent_to_email, is_consent_to_call, is_consent_to_sms: Flags indicating client's consent to various forms of communication.\n",
        "\n",
        "• is_valid_dm, is_valid_email: Flags indicating the validity of direct mail and email addresses.\n",
        "\n",
        "• is_housewife_retiree, is_sg_pr, is_class_1_2: Flags indicating specific demographics like occupation, residency status, etc.\n",
        "\n",
        "• is_dependent_in_at_least_1_policy: Indicates if the client is a dependent in at least one policy.\n",
        "\n",
        "For these flags, we noted the same issues as in the previous flags. Upon further checking, these NA values are from the same rows as well, which may be indicative of some underlying issues. Therefore, this may affect the rationale of filling in the null values afterwards and should be taken note.\n",
        "\n",
        "• hh_20, pop_20, hh_size, hh_size_est: Metrics related to household size and population.\n",
        "\n",
        "In terms of the distribution for hh_20, pop_20 and hh_size, it turns out that all these columns have similar distributions. Upon further check, the hh_size column is a function of the hh_20 column and pop_20 column. Thus, it may be wise to consider dropping those two columns as they may lead to multicollinearity between columns. The hh_size_est is just a round up of the hh_size column in front. From the observations, the majority bucket for household size is 3, while the least is 0. All 4 columns have 2809 missing values, which should be taken note in future steps.\n",
        "\n",
        "• annual_income_est: Estimated annual income of the client, in buckets.\n",
        "\n",
        "The annual income buckets have customers below 30k Income as the largest group. Similar to household columns, the annual income column has 2809 missing values as well, which should be taken note in future steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O5hCINDcTz7a"
      },
      "outputs": [],
      "source": [
        "lsOfCols = ['is_consent_to_mail', 'is_consent_to_email','is_consent_to_call', 'is_consent_to_sms', 'is_valid_dm','is_valid_email', 'is_housewife_retiree', 'is_sg_pr', 'is_class_1_2', 'is_dependent_in_at_least_1_policy']\n",
        "flagDist = pd.DataFrame(columns=['Zero', 'One', 'NA value'])\n",
        "for col in lsOfCols:\n",
        "  zero, one, NA = show_dist(col)\n",
        "  new_row = {'Zero': zero, 'One': one, 'NA value': NA}\n",
        "  flagDist = pd.concat([flagDist, pd.DataFrame(new_row, index=[col])])\n",
        "flagDist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "92mHk03lQKls"
      },
      "outputs": [],
      "source": [
        "df.hh_20.dropna().astype(int).plot(kind='kde')\n",
        "plt.title('hh_20 distribution')\n",
        "df.hh_20.dropna().astype(int).describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Csvx2J6TjV-F"
      },
      "outputs": [],
      "source": [
        "df.pop_20.dropna().astype(int).plot(kind='kde')\n",
        "plt.title('Pop_20 distribution')\n",
        "df.pop_20.dropna().astype(int).describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XZBkBYbSjdXW"
      },
      "outputs": [],
      "source": [
        "df.hh_size.dropna().astype(int).plot(kind='kde')\n",
        "plt.title('Household size distribution')\n",
        "df.hh_size.dropna().astype(int).describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SuyCgNezlMHT"
      },
      "outputs": [],
      "source": [
        "print(f'Number of NA values : {df.hh_20.isna().sum()}')\n",
        "print(f'Number of NA values : {df.pop_20.isna().sum()}')\n",
        "print(f'Number of NA values : {df.hh_size.isna().sum()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5O5_JMNDj6rg"
      },
      "outputs": [],
      "source": [
        "# hh_size_est - household size round\n",
        "print(f'Number of NA values : {df.hh_size_est.isna().sum()}')\n",
        "hh_size_est = df.hh_size_est.value_counts().reset_index()\n",
        "plt.figure(figsize=(8, 8))\n",
        "ax = sns.barplot(x='index', y='hh_size_est', data=hh_size_est)\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
        "plt.title('Household Size Estimate Distribution')\n",
        "plt.xlabel('')\n",
        "plt.ylabel('')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ivb198QbkYs9"
      },
      "outputs": [],
      "source": [
        "# annual_income_est - household size round\n",
        "print(f'Number of NA values : {df.annual_income_est.isna().sum()}')\n",
        "annual_income_est = df.annual_income_est.value_counts().reset_index()\n",
        "plt.figure(figsize=(8, 8))\n",
        "ax = sns.barplot(x='index', y='annual_income_est', data=annual_income_est)\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
        "plt.title('Annual Income Estimate Distribution')\n",
        "plt.xlabel('')\n",
        "plt.ylabel('')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzkDrFF9mQbU"
      },
      "source": [
        "##### Policy and Claim History\n",
        "\n",
        "• n_months_last_bought_products, flg_latest_being_lapse, flg_latest_being_cancel, recency_lapse, recency_cancel: Metrics related to the recency of policy purchases, lapses, and cancellations.\n",
        "\n",
        "There are up to 32 types of product from which the n months are recorded. The n_months_last_bought_products is a compilation of the user's activity in purchasing insurance in previous time steps. Thus, there may be some multicollinearity that is embedded in the columns and thus should be taken note.\n",
        "\n",
        "• tot_inforce_pols, tot_cancel_pols: Total number of in-force and canceled policies.\n",
        "\n",
        "• f_ever_declined_la: Flag for clients has ever been declined policies.\n",
        "\n",
        "For columns flg_latest_being_lapse and flg_latest_being_cancel, the columns are binary flags and have no null values. These may be important indicators for the columns For other columns, these are flags for the"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-16S-KmsmXG3"
      },
      "outputs": [],
      "source": [
        "# lsOfColnames = [\"ape_lh\", \"prempaid_\", \"flg_hlthclaim_\", \"lapse_ape_grp\", \"n_months_since_lapse_\", \"hlthclaim_amt\", \"giclaim_amt\", \"recency_hlthclaim\", \"recency_giclaim\", \"flg_affconnect_\", \"f_hold\", \"n_months_last_bought_\", \"sumins_\"]\n",
        "lsOfColnames = [\"n_months_last_bought_\", \"flg_latest_being_lapse\", \"flg_latest_being_cancel\", \"recency_lapse\", \"recency_cancel\", \"tot_inforce_pols\", \"tot_cancel_pols\", \"f_ever_declined_la\"]\n",
        "for c in lsOfColnames:\n",
        "    print(f'Number of {c} columns in the dataset : {len([col for col in df.columns if (c in col)])}' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5N-_sl6BsFcA"
      },
      "outputs": [],
      "source": [
        "print(f'Number of NA values : {df.n_months_last_bought_products.isna().sum()}')\n",
        "df.n_months_last_bought_products.dropna().astype(int).plot(kind='kde')\n",
        "plt.title('n_months_last_bought_products distribution')\n",
        "df.n_months_last_bought_products.dropna().astype(int).describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4P1VF7ubqLJt"
      },
      "outputs": [],
      "source": [
        "# flg_latest_being_lapse distribution\n",
        "print(f'Number of NA values : {df.flg_latest_being_lapse.isna().sum()}')\n",
        "flg_latest_being_lapse = df.flg_latest_being_lapse.value_counts().reset_index()\n",
        "plt.figure(figsize=(8, 8))\n",
        "ax = sns.barplot(x='index', y='flg_latest_being_lapse', data=flg_latest_being_lapse)\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
        "plt.title('flg_latest_being_lapse Distribution')\n",
        "plt.xlabel('')\n",
        "plt.ylabel('')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vAdF9cMlqWmG"
      },
      "outputs": [],
      "source": [
        "# flg_latest_being_cancel distribution\n",
        "print(f'Number of NA values : {df.flg_latest_being_cancel.isna().sum()}')\n",
        "flg_latest_being_cancel = df.flg_latest_being_cancel.value_counts().reset_index()\n",
        "plt.figure(figsize=(8, 6))\n",
        "ax = sns.barplot(x='index', y='flg_latest_being_cancel', data=flg_latest_being_cancel)\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
        "plt.title('flg_latest_being_cancel Distribution')\n",
        "plt.xlabel('')\n",
        "plt.ylabel('')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OxuI16zNqWuT"
      },
      "outputs": [],
      "source": [
        "print(f'Number of NA values : {df.recency_cancel.isna().sum()}')\n",
        "df.recency_cancel.dropna().astype(int).plot(kind='kde')\n",
        "plt.title('recency_cancel distribution')\n",
        "df.recency_cancel.dropna().astype(int).describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "li4J3_QtqZDP"
      },
      "outputs": [],
      "source": [
        "print(f'Number of NA values : {df.recency_lapse.isna().sum()}')\n",
        "df.recency_lapse.dropna().astype(int).plot(kind='kde')\n",
        "plt.title('recency_lapse distribution')\n",
        "df.recency_lapse.dropna().astype(int).describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GVyzdhf8rgSM"
      },
      "outputs": [],
      "source": [
        "# \"tot_inforce_pols\", \"tot_cancel_pols\", \"f_ever_declined_la\"\n",
        "print(f'Number of NA values : {df.tot_inforce_pols.isna().sum()}')\n",
        "df.tot_inforce_pols.dropna().astype(int).plot(kind='kde')\n",
        "plt.title('tot_inforce_pols distribution')\n",
        "df.tot_inforce_pols.dropna().astype(int).describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wJaZvG35rnfa"
      },
      "outputs": [],
      "source": [
        "print(f'Number of NA values : {df.tot_cancel_pols.isna().sum()}')\n",
        "df.tot_cancel_pols.dropna().astype(int).plot(kind='kde')\n",
        "plt.title('tot_cancel_pols distribution')\n",
        "df.tot_cancel_pols.dropna().astype(int).describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FCnABejVrnh3"
      },
      "outputs": [],
      "source": [
        "print(f'Number of NA values : {df.f_ever_declined_la.isna().sum()}')\n",
        "f_ever_declined_la = df.f_ever_declined_la.value_counts().reset_index()\n",
        "plt.figure(figsize=(8, 6))\n",
        "ax = sns.barplot(x='index', y='f_ever_declined_la', data=f_ever_declined_la)\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
        "plt.title('f_ever_declined_la Distribution')\n",
        "plt.xlabel('')\n",
        "plt.ylabel('')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oKvP5mjmrEU"
      },
      "source": [
        "##### Anonymized Insurance Product Metrics (APE, Sum Insured, Prepaid Premiums)\n",
        "\n",
        "• ape_, sumins_, prempaid_* (e.g., ape_gi_42e115, sumins_ltc_1280bf, prempaid_grp_6fc3e6): Metrics for various anonymized insurance products, likely representing different types of policies like general insurance, long-term care, group policies, etc. The suffixes (like 42e115, 1280bf) are unique identifiers for the specific insurance products. ‘ape’ stands for Annual Premium Equivalent, 'sumins' for sum insured, ‘prempaid’ stands for premium customers will pay from product inception to product maturity.\n",
        "\n",
        "Making up for the bulk of the columns in the dataset, we do notice that these columns have similar patterns and datatype. This is important to note and great news when it comes to data preprocessing steps in the future.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ej2oc2m8rmSR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rAql2lpVnhVN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pX6uLqjxniE8"
      },
      "source": [
        "##### Other Flags and Metrics\n",
        "\n",
        "• f_elx, f_mindef_mha, f_retail: Flags possibly related to client's association with specific programs or sectors.\n",
        "\n",
        "• flg_affconnect_*, affcon_visit_days, n_months_since_visit_affcon: Metrics related to client’s activity in affinity connect.\n",
        "\n",
        "• clmcon_visit_days, recency_clmcon, recency_clmcon_regis: Metrics related to client’s activity in claim connect.\n",
        "\n",
        "• hlthclaim_amt, giclaim_amt, recency_hlthclaim, recency_giclaim, hlthclaim_cnt_success, giclaim_cnt_success: Health and general insurance claim-related metrics.\n",
        "\n",
        "• flg_hlthclaim_, flg_gi_claim_ (e.g., flg_hlthclaim_839f8a_ever, flg_gi_claim_29d435_ever): Flags for specific types of health and general insurance claims, with anonymized identifiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MAo6ROten4Sz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm2nes9Kn8Hv"
      },
      "source": [
        "##### Purchase and Lapse Metrics for Specific Products\n",
        "\n",
        "• f_ever_bought_, n_months_last_bought_, lapse_ape_, n_months_since_lapse_ (e.g., f_ever_bought_839f8a, n_months_last_bought_grp_6fc3e6, lapse_ape_ltc_1280bf, n_months_since_lapse_inv_dcd836): Flags and metrics indicating purchase history, lapses, and time since last interaction for various anonymized insurance products..\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JKlI6F2stwVj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nFrzycuBul9"
      },
      "source": [
        "#### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "z0Wt7MlCOZPQ"
      },
      "outputs": [],
      "source": [
        "# read file\n",
        "# df = pq.read_table('catB_train.parquet').to_pandas()\n",
        "\n",
        "# filter out anomylised filters first  or ('n_months_last_bought_' in col) hlthclaim_cnt_success giclaim_cnt_success recency_clmcon_regis affcon_visit_days n_months_since_visit_affcon\n",
        "anomylisedIdentifiers = [col for col in df.columns if ('ape_' in col) or ('sumins_' in col) or ('prempaid_' in col) or ('flg_hlthclaim_' in col) or ('flg_gi_claim_' in col) or ('f_ever_bought_' in col)\n",
        "                                                                           or ('n_months_last_bought_' in col) or ('lapse_ape_' in col) or ('n_months_since_lapse_' in col) or ('hlthclaim_amt' in col) or ('giclaim_amt' in col) or ('recency_hlthclaim' in col)\n",
        "                                                                           or ('recency_giclaim' in col)\n",
        "                                                                            or ('flg_affconnect_' in col) or ('f_hold' in col)]\n",
        "# generalInfo = [col for col in df.columns if col not in anomylisedIdentifiers] + ['n_months_last_bought_products']\n",
        "# all columns\n",
        "# ['stat_flag', 'min_occ_date', 'cltdob_fix', 'flg_substandard',\n",
        "#        'flg_is_borderline_standard', 'flg_is_revised_term',\n",
        "#        'flg_is_rental_flat', 'flg_has_health_claim', 'flg_has_life_claim',\n",
        "#        'flg_gi_claim', 'flg_is_proposal', 'flg_with_preauthorisation',\n",
        "#        'flg_is_returned_mail', 'is_consent_to_mail', 'is_consent_to_email',\n",
        "#        'is_consent_to_call', 'is_consent_to_sms', 'is_valid_dm',\n",
        "#        'is_valid_email', 'is_housewife_retiree', 'is_sg_pr', 'is_class_1_2',\n",
        "#        'is_dependent_in_at_least_1_policy', 'f_ever_declined_la',\n",
        "#        'hh_size_est', 'annual_income_est', 'flg_latest_being_lapse',\n",
        "#        'flg_latest_being_cancel', 'recency_lapse', 'recency_cancel',\n",
        "#        'tot_inforce_pols', 'tot_cancel_pols', 'f_elx', 'f_mindef_mha',\n",
        "#        'f_retail', 'affcon_visit_days', 'n_months_since_visit_affcon',\n",
        "#        'clmcon_visit_days', 'recency_clmcon', 'recency_clmcon_regis', 'race_desc', 'clttype', 'ctrycode_desc', 'cltsex_fix',\n",
        "#        'stat_flag', 'annual_income_est', 'hh_size_est']\n",
        "\n",
        "columns = ['clntnum', 'stat_flag', 'min_occ_date', 'cltdob_fix', 'flg_substandard',\n",
        "       'flg_is_borderline_standard', 'flg_is_revised_term',\n",
        "       'flg_is_rental_flat', 'flg_has_health_claim', 'flg_has_life_claim',\n",
        "       'flg_gi_claim', 'flg_is_proposal', 'flg_with_preauthorisation',\n",
        "       'flg_is_returned_mail', 'is_consent_to_mail', 'is_consent_to_email',\n",
        "       'is_consent_to_call', 'is_consent_to_sms', 'is_valid_dm',\n",
        "       'is_valid_email', 'is_housewife_retiree', 'is_sg_pr', 'is_class_1_2',\n",
        "       'is_dependent_in_at_least_1_policy', 'f_ever_declined_la',\n",
        "       'hh_size_est', 'annual_income_est', 'flg_latest_being_lapse',\n",
        "       'flg_latest_being_cancel',\n",
        "       'tot_inforce_pols', 'f_elx', 'f_mindef_mha',\n",
        "       'f_retail', 'affcon_visit_days', 'recency_clmcon_regis', 'race_desc', 'clttype', 'ctrycode_desc', 'cltsex_fix', 'f_purchase_lh', 'hh_20', 'pop_20', 'hh_size']\n",
        "\n",
        "################ Preprocessing for Additional Information part\n",
        "additionalInfo = df[anomylisedIdentifiers + ['f_purchase_lh']]\n",
        "insuranceRelated = [col for col in df.columns if ('ape_' in col) or ('sumins_' in col) or ('prempaid_' in col) or ('flg_hlthclaim_' in col) or ('flg_gi_claim_' in col) or ('f_ever_' in col)\n",
        "                    or ('n_months_last_bought_' in col) or ('lapse_ape_' in col) or ('n_months_since_lapse_' in col) or ('hlthclaim_amt' in col) or ('giclaim_amt' in col) or ('recency_hlthclaim' in col)\n",
        "                    or ('recency_giclaim' in col) or ('hlthclaim_cnt_success' in col) or ('giclaim_cnt_success' in col)\n",
        "                    or ('flg_affconnect_' in col) or ('affcon_visit_days' in col) or ('n_months_since_visit_affcon' in col) or ('f_hold' in col)]\n",
        "\n",
        "# preprocessing for anomylised columns\n",
        "df['ape_overview'] = df[[col for col in df.columns if ('ape' in col)]].fillna(0).sum(axis=1)\n",
        "for each in [col for col in df.columns if ('n_months_since_lapse_' in col)]:\n",
        "    df[f'{each}'] = df[f'{each}'].apply(lambda x: 0 if x=='9999' or pd.isna(x) else float(x))\n",
        "df['nmonths_overview'] = df[[col for col in df.columns if ('n_months_since_lapse_' in col)]].fillna(0).sum(axis=1)\n",
        "df['prempaid_overview'] = df[[col for col in df.columns if ('prempaid_' in col)]].fillna(0).sum(axis=1)\n",
        "df['fhold_overview'] = df[[col for col in df.columns if ('f_hold' in col)]].fillna(0).sum(axis=1)\n",
        "df['fever_overview'] = df[[col for col in df.columns if ('f_ever' in col)]].fillna(0).sum(axis=1)\n",
        "df['hlthclaim_overview'] = df[[col for col in df.columns if ('recency_hlthclaim' in col)]].fillna(0).sum(axis=1)\n",
        "df['fig_overview'] = df[[col for col in df.columns if ('flg_affconnect_' in col)]].fillna(0).sum(axis=1)\n",
        "df['giclaim_overview'] = df[[col for col in df.columns if ('recency_giclaim' in col)]].fillna(0).sum(axis=1)\n",
        "df['sumins_overview'] = df[[col for col in df.columns if ('sumins_' in col)]].fillna(0).sum(axis=1)\n",
        "\n",
        "df.prempaid_overview = df.prempaid_overview.astype(int).apply(lambda x: np.log(x+1))\n",
        "df.ape_overview = df.ape_overview.astype(int).apply(lambda x: np.log(x+1))\n",
        "# df.nmonths_overview = df.nmonths_overview.astype(int).apply(lambda x: np.log(x+1)).fillna(0) # nmonths contains negative values\n",
        "df.sumins_overview = df.sumins_overview.astype(int).apply(lambda x: np.log(x+1))\n",
        "additionalColumns = ['ape_overview', 'nmonths_overview', 'prempaid_overview', 'fhold_overview', 'fever_overview', 'hlthclaim_overview', 'fig_overview', 'giclaim_overview', 'sumins_overview']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qAfo8hAYwjCU"
      },
      "outputs": [],
      "source": [
        "pca = PCA()  # Set the desired number of components\n",
        "insuranceDf = pca.fit_transform(df[insuranceRelated].fillna(0))\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "plt.plot(explained_variance_ratio, marker='o')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('Explained Variance Ratio vs. Number of Components')\n",
        "plt.show()\n",
        "cumulative_explained_variance = explained_variance_ratio.cumsum()\n",
        "plt.plot(cumulative_explained_variance, marker='o')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Cumulative Explained Variance vs. Number of Components')\n",
        "plt.show()\n",
        "desired_variance_ratio = 0.95  # Set your desired explained variance ratio\n",
        "n_components = np.argmax(cumulative_explained_variance >= desired_variance_ratio) + 1\n",
        "print(f\"Number of components to retain {desired_variance_ratio * 100}% of variance: {n_components}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "69h_EpaMwqWe"
      },
      "outputs": [],
      "source": [
        "additionalDf = pd.concat([df[additionalColumns].reset_index().drop('index', axis=1), pd.DataFrame(insuranceDf)], axis=1)\n",
        "# merge additional columns with insurance df\n",
        "#############\n",
        "\n",
        "generalInfo = [col for col in df.columns if col not in anomylisedIdentifiers] + ['n_months_last_bought_products']\n",
        "generalInfo = [col for col in generalInfo if col not in ['recency_cancel', 'tot_cancel_pols', 'clmcon_visit_days', 'recency_clmcon']]\n",
        "# generalInfo = columns\n",
        "\n",
        "# create a general copy\n",
        "general = df[generalInfo].reset_index().drop('index', axis=1)\n",
        "generalCopy = general.copy() # make a copy of the original dataset\n",
        "\n",
        "# concat with the additionalDf set\n",
        "general = pd.concat([general.reset_index().drop('index', axis=1), pd.DataFrame(insuranceDf)], axis=1)\n",
        "\n",
        "# # data cleaning for general columns\n",
        "# f_purchase_lh - fix purchase (target) as 0 for null value\n",
        "general.f_purchase_lh = general.f_purchase_lh.fillna(0)\n",
        "\n",
        "# race - fillna with undisclosed\n",
        "general.race_desc = general.race_desc.fillna('Undisclosed')\n",
        "\n",
        "# gender - fillna as undisclosed\n",
        "general.cltsex_fix = general.cltsex_fix.fillna(\"Undisclosed\")\n",
        "\n",
        "# n_months_last_bought_products - fillna as 0\n",
        "general.n_months_last_bought_products = general.n_months_last_bought_products.fillna(0)\n",
        "\n",
        "# all flags - fillna as -1 to seperate from the provided values, non submission in flags may be indicative of not purchasing\n",
        "# all is_ binary attributes - fillna as -1 to seperate from the provided values, non submission in any of the attributes may be indicative of not purchasing\n",
        "def fillNA(value):\n",
        "    general.flg_substandard = general.flg_substandard.fillna(value)\n",
        "    general.flg_is_borderline_standard = general.flg_is_borderline_standard.fillna(value)\n",
        "    general.flg_is_revised_term = general.flg_is_revised_term.fillna(value)\n",
        "    general.flg_has_life_claim = general.flg_has_life_claim.fillna(value)\n",
        "    general.flg_gi_claim = general.flg_gi_claim.fillna(value)\n",
        "    general.flg_is_proposal = general.flg_is_proposal.fillna(value)\n",
        "    general.flg_with_preauthorisation = general.flg_with_preauthorisation.fillna(value)\n",
        "    general.flg_is_rental_flat = general.flg_is_rental_flat.fillna(value)\n",
        "    general.flg_has_health_claim = general.flg_has_health_claim.fillna(value)\n",
        "    general.flg_is_returned_mail = general.flg_is_returned_mail.fillna(value)\n",
        "\n",
        "    general.is_consent_to_mail = general.is_consent_to_mail.fillna(value)\n",
        "    general.is_consent_to_email = general.is_consent_to_email.fillna(value)\n",
        "    general.is_consent_to_call = general.is_consent_to_call.fillna(value)\n",
        "    general.is_consent_to_sms = general.is_consent_to_sms.fillna(value)\n",
        "    general.is_valid_dm = general.is_valid_dm.fillna(value)\n",
        "    general.is_valid_email = general.is_valid_email.fillna(value)\n",
        "    general.is_housewife_retiree = general.is_housewife_retiree.fillna(value)\n",
        "    general.is_sg_pr = general.is_sg_pr.fillna(value)\n",
        "    general.is_class_1_2 = general.is_class_1_2.fillna(value)\n",
        "    general.is_dependent_in_at_least_1_policy = general.is_dependent_in_at_least_1_policy.fillna(value)\n",
        "\n",
        "fillNA(-1)\n",
        "\n",
        "# giclaim_cnt_unsuccess - array of nulls, to be removed\n",
        "general = general.drop('giclaim_cnt_unsuccess', axis=1)\n",
        "\n",
        "# hlthclaim_cnt_unsuccess + hlthclaim_cnt_success - transform column into either tried to claim before or not\n",
        "general.hlthclaim_cnt_unsuccess = general.hlthclaim_cnt_unsuccess.fillna(0)\n",
        "general.hlthclaim_cnt_success = general.hlthclaim_cnt_success.fillna(0)\n",
        "general['made_unsuccess_hlthclaim'] = general.hlthclaim_cnt_unsuccess.apply(lambda x: 1 if x!=0 else 0)\n",
        "general['made_success_hlthclaim'] = general.hlthclaim_cnt_success.apply(lambda x: 1 if x!=0 else 0)\n",
        "general = general.drop(['hlthclaim_cnt_unsuccess', 'hlthclaim_cnt_success'], axis=1)\n",
        "general.giclaim_cnt_success = general.giclaim_cnt_success.fillna(0)\n",
        "general.giclaim_cnt_success = general.giclaim_cnt_success.apply(lambda x: 1 if x!=0 else 0)\n",
        "general.recency_clmcon_regis = general.recency_clmcon_regis.fillna(0)\n",
        "general.recency_clmcon_regis = general.recency_clmcon_regis.apply(lambda x: 1 if x!=0 else 0)\n",
        "general.affcon_visit_days = general.affcon_visit_days.fillna(0)\n",
        "general.affcon_visit_days = general.affcon_visit_days.apply(lambda x: 1 if x!=0 else 0)\n",
        "general.n_months_since_visit_affcon = general.n_months_since_visit_affcon.fillna(0)\n",
        "general.n_months_since_visit_affcon = general.n_months_since_visit_affcon.apply(lambda x: 1 if x!=0 else 0)\n",
        "\n",
        "# f_ever_declined_la - fill in 0 for null values\n",
        "general.f_ever_declined_la = general.f_ever_declined_la.fillna(0)\n",
        "\n",
        "# hh_size_est - keep only hh_size_est, drop hh_20, pop_20, hh_size as hh_size_est is derived from the three, fillna as -1 to differentiate from given value\n",
        "general.hh_size_est = general.hh_size_est.fillna('-1')\n",
        "general = general.drop(['hh_20', 'pop_20', 'hh_size'], axis=1)\n",
        "\n",
        "# annual_income_est -  fillna as -1 to differentiate from given value\n",
        "general.annual_income_est = general.annual_income_est.fillna(\"F-UNDISCLOSED\")\n",
        "\n",
        "# recency_lapse - fillna with median\n",
        "general.recency_lapse = general.recency_lapse.fillna(general.recency_lapse.median())\n",
        "\n",
        "# LARGE AMOUNT OF MISSING DATA - recency_cancel, tot_cancel_pols, clmcon_visit_days, recency_clmcon\n",
        "# # recency_cancel - convert to binary as the amount of data missing is too much\n",
        "# general.recency_cancel = general.recency_cancel.apply(lambda x : 0 if pd.isna(x) else 1)\n",
        "\n",
        "# # tot_cancel_pols - convert to binary as the amount of data missing is too much\n",
        "# general.tot_cancel_pols = general.tot_cancel_pols.apply(lambda x : 0 if pd.isna(x) else 1)\n",
        "\n",
        "# # clmcon_visit_days, recency_clmcon - convert to binary as the amount of data missing is too much\n",
        "# general.clmcon_visit_days = general.clmcon_visit_days.apply(lambda x : 0 if pd.isna(x) else 1)\n",
        "# general.recency_clmcon = general.recency_clmcon.apply(lambda x : 0 if pd.isna(x) else 1)\n",
        "\n",
        "# drop the clntnum column\n",
        "general = general.drop('clntnum', axis=1)\n",
        "\n",
        "# # preprocess text columns - clttype, ctrycode_desc, stat_flag, cltsex_fix, annual_income_est, hh_size_est\n",
        "# one hot encoding for nominal columns - clttype, ctrycode_desc, cltsex_fix\n",
        "encoder = OneHotEncoder()\n",
        "one_hot_encoded = encoder.fit_transform(general[['race_desc', 'clttype', 'ctrycode_desc', 'cltsex_fix']])\n",
        "one_hot_df = pd.DataFrame(one_hot_encoded.toarray(), columns=encoder.get_feature_names_out(['race_desc', 'clttype', 'ctrycode_desc', 'cltsex_fix']))\n",
        "general = pd.concat([general, one_hot_df], axis=1)\n",
        "general = general.drop(['race_desc', 'clttype', 'ctrycode_desc', 'cltsex_fix'], axis=1)\n",
        "\n",
        "# label encoding for ordinal columns - annual_income_est, stat_flag, hh_size_est\n",
        "custom_order = ['ACTIVE', 'LAPSED', 'MATURED']\n",
        "ordinal_encoder = OrdinalEncoder(categories=[custom_order])\n",
        "general['stat_flag'] = ordinal_encoder.fit_transform(general[['stat_flag']])\n",
        "\n",
        "custom_order = ['F-UNDISCLOSED', 'E.BELOW30K', 'D.30K-60K', 'C.60K-100K', 'B.100K-200K', 'A.ABOVE200K']\n",
        "ordinal_encoder = OrdinalEncoder(categories=[custom_order])\n",
        "general['annual_income_est'] = ordinal_encoder.fit_transform(general[['annual_income_est']])\n",
        "\n",
        "custom_order = ['-1', '0', '1', '2', '3', '4', '>4']\n",
        "ordinal_encoder = OrdinalEncoder(categories=[custom_order])\n",
        "general['hh_size_est'] = ordinal_encoder.fit_transform(general[['hh_size_est']])\n",
        "\n",
        "# preprocess the datetime columns - retain delta of years for information\n",
        "general.min_occ_date = general.min_occ_date.apply(lambda x : x[:4])\n",
        "general.cltdob_fix = general.cltdob_fix.apply(lambda x : x[:4])\n",
        "\n",
        "# min_occ_date -  replace with median (8.0) if null value observed\n",
        "general.min_occ_date =  general.min_occ_date.replace('None', pd.NA)\n",
        "general.min_occ_date = general.min_occ_date.apply(lambda x: 8.0 if pd.isna(x) else 2024 - int(x))\n",
        "\n",
        "# for cltdob_fix - replace with median (45) if null value observed\n",
        "general.cltdob_fix =  general.cltdob_fix.replace('None', pd.NA)\n",
        "general.cltdob_fix = general.cltdob_fix.apply(lambda x: 45 if pd.isna(x) else 2024 - int(x))\n",
        "\n",
        "# stratified train test split\n",
        "X, y = general.drop('f_purchase_lh', axis=1), general['f_purchase_lh']\n",
        "X.columns = X.columns.astype(str)\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# oversampling for training set\n",
        "# XtrainOversampled, ytrainOversampled = SMOTE(random_state=42).fit_resample(Xtrain, ytrain)\n",
        "adasyn = ADASYN(sampling_strategy=0.5, random_state=42)\n",
        "XtrainOversampled, ytrainOversampled = adasyn.fit_resample(Xtrain, ytrain)\n",
        "\n",
        "# resampled for training\n",
        "# Use RandomUnderSampler to handle class imbalance for the majority class (0)\n",
        "undersampler = RandomUnderSampler(sampling_strategy=0.3, random_state=42)  # Adjust the sampling_strategy as needed\n",
        "XtrainOversampled, ytrainOversampled = undersampler.fit_resample(Xtrain, ytrain)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fwgKO2MUxfzy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fi-QovF0xf2V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YGJEeshxngy"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0JQyWE2x_Vu"
      },
      "source": [
        "#### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KL3Ruj7Axf-s"
      },
      "outputs": [],
      "source": [
        "# Train an XGBoost classifier with regularization\n",
        "xgb_model = XGBClassifier(\n",
        "    learning_rate=0.1,\n",
        "    scale_pos_weight=0.9,\n",
        "    max_depth=3,\n",
        "    min_child_weight=3,\n",
        "    reg_alpha=0.41,  # L1 regularization term\n",
        "    reg_lambda=1.5\n",
        ")\n",
        "\n",
        "# Fit the model on the training set\n",
        "xgb_model.fit(XtrainOversampled, ytrainOversampled)\n",
        "\n",
        "# Make predictions on the training set\n",
        "y_train_pred = xgb_model.predict(XtrainOversampled)\n",
        "\n",
        "# Calculate ROC-AUC score on the training set\n",
        "roc_auc_train = roc_auc_score(ytrainOversampled, xgb_model.predict_proba(XtrainOversampled)[:, 1])\n",
        "\n",
        "# Evaluate model performance on the training set\n",
        "accuracy_train = accuracy_score(ytrainOversampled, y_train_pred)\n",
        "report_train = classification_report(ytrainOversampled, y_train_pred)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = xgb_model.predict(Xtest)\n",
        "xgb_predProba = xgb_model.predict_proba(Xtest)[:, 1]\n",
        "\n",
        "# Calculate ROC-AUC score on the test set\n",
        "roc_auc = roc_auc_score(ytest, xgb_model.predict_proba(Xtest)[:, 1])\n",
        "\n",
        "# Evaluate model performance on the test set\n",
        "accuracy = accuracy_score(ytest, y_pred)\n",
        "report = classification_report(ytest, y_pred)\n",
        "\n",
        "# Calculate F1 score on the training set\n",
        "f1_train = f1_score(ytrainOversampled, y_train_pred)\n",
        "\n",
        "# Calculate F1 score on the test set\n",
        "f1_test = f1_score(ytest, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LsRad3i0xxlb"
      },
      "outputs": [],
      "source": [
        "# Print F1 scores\n",
        "print(\"\\nTraining Set F1 Score:\", f1_train)\n",
        "print(\"Test Set F1 Score:\", f1_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "R5pVsGTrxxod"
      },
      "outputs": [],
      "source": [
        "# Print metrics for both training and test sets\n",
        "print(\"\\nTraining Set Metrics:\")\n",
        "print(f\"Accuracy: {accuracy_train}\")\n",
        "print(f\"ROC-AUC: {roc_auc_train}\")\n",
        "print(\"Classification Report:\\n\", report_train)\n",
        "\n",
        "print(\"\\nTest Set Metrics:\")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"ROC-AUC: {roc_auc}\")\n",
        "print(\"Classification Report:\\n\", report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PavDGd_pxxqi"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
        "\n",
        "# Confusion matrix for training data\n",
        "sns.heatmap(confusion_matrix(ytrainOversampled, y_train_pred),\n",
        "            annot=True, fmt=\".0f\", annot_kws={\"size\": 18}, ax=axes[0])\n",
        "axes[0].set_title('Train Data')\n",
        "\n",
        "# Confusion matrix for test data\n",
        "sns.heatmap(confusion_matrix(ytest, y_pred),\n",
        "            annot=True, fmt=\".0f\", annot_kws={\"size\": 18}, ax=axes[1])\n",
        "axes[1].set_title('Test Data')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pZAdiKVcxxsh"
      },
      "outputs": [],
      "source": [
        "print(\"Train Data\")\n",
        "\n",
        "accuracy_train = accuracy_score(ytrainOversampled, y_train_pred)\n",
        "precision_train = precision_score(ytrainOversampled, y_train_pred, average='binary')\n",
        "recall_train = recall_score(ytrainOversampled, y_train_pred, average='binary')\n",
        "f1_train = f1_score(ytrainOversampled, y_train_pred, average='binary')\n",
        "print(f\"Accuracy: {accuracy_train}\")\n",
        "print(f\"Precision: {precision_train}\")\n",
        "print(f\"Recall: {recall_train}\")\n",
        "print(f\"F1 Score: {f1_train}\")\n",
        "print()\n",
        "print(\"Test Data\")\n",
        "\n",
        "accuracy_test = accuracy_score(ytest, y_pred)\n",
        "precision_test = precision_score(ytest, y_pred, average='binary')\n",
        "recall_test = recall_score(ytest, y_pred, average='binary')\n",
        "f1_test = f1_score(ytest, y_pred, average='binary')\n",
        "print(f\"Accuracy: {accuracy_test}\")\n",
        "print(f\"Precision: {precision_test}\")\n",
        "print(f\"Recall: {recall_test}\")\n",
        "print(f\"F1 Score: {f1_test}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lNNwxChmyDPz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL35Hb8RyDm4"
      },
      "source": [
        "#### AdaBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Fa7qUlBDxxvX"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "base_estimator = DecisionTreeClassifier()\n",
        "\n",
        "ada_boost_model = AdaBoostClassifier(base_estimator=base_estimator, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'base_estimator__max_depth': [1, 2, 3],\n",
        "    'n_estimators': [20, 50, 100],\n",
        "    'learning_rate': [0.01, 0.1, 1]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=ada_boost_model, param_grid=param_grid, cv=2, scoring='f1', verbose=1, n_jobs=-1)\n",
        "\n",
        "\n",
        "grid_search.fit(XtrainOversampled, ytrainOversampled)\n",
        "\n",
        "# Best parameters\n",
        "best_ada_params = grid_search.best_params_\n",
        "print(\"Best Parameters:\", best_ada_params)\n",
        "\n",
        "ada_model = grid_search.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1htFmgpTxxxR"
      },
      "outputs": [],
      "source": [
        "# Fit the model on the training set\n",
        "ada_model.fit(XtrainOversampled, ytrainOversampled)\n",
        "\n",
        "# Make predictions on the training set\n",
        "y_train_pred = ada_model.predict(XtrainOversampled)\n",
        "\n",
        "# Calculate ROC-AUC score on the training set\n",
        "roc_auc_train = roc_auc_score(ytrainOversampled, ada_model.predict_proba(XtrainOversampled)[:, 1])\n",
        "\n",
        "# Evaluate model performance on the training set\n",
        "accuracy_train = accuracy_score(ytrainOversampled, y_train_pred)\n",
        "report_train = classification_report(ytrainOversampled, y_train_pred)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ada_model.predict(Xtest)\n",
        "ada_predProba = ada_model.predict_proba(Xtest)[:, 1]\n",
        "\n",
        "# Calculate ROC-AUC score on the test set\n",
        "roc_auc = roc_auc_score(ytest, ada_model.predict_proba(Xtest)[:, 1])\n",
        "\n",
        "# Evaluate model performance on the test set\n",
        "accuracy = accuracy_score(ytest, y_pred)\n",
        "report = classification_report(ytest, y_pred)\n",
        "\n",
        "# Calculate F1 score on the training set\n",
        "f1_train = f1_score(ytrainOversampled, y_train_pred)\n",
        "\n",
        "# Calculate F1 score on the test set\n",
        "f1_test = f1_score(ytest, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ux8Gpa_Rxxzj"
      },
      "outputs": [],
      "source": [
        "# Print F1 scores\n",
        "print(\"\\nTraining Set F1 Score:\", f1_train)\n",
        "print(\"Test Set F1 Score:\", f1_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_KTbWGzKxx1X"
      },
      "outputs": [],
      "source": [
        "# Print metrics for both training and test sets\n",
        "print(\"\\nTraining Set Metrics:\")\n",
        "print(f\"Accuracy: {accuracy_train}\")\n",
        "print(f\"ROC-AUC: {roc_auc_train}\")\n",
        "print(\"Classification Report:\\n\", report_train)\n",
        "\n",
        "print(\"\\nTest Set Metrics:\")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"ROC-AUC: {roc_auc}\")\n",
        "print(\"Classification Report:\\n\", report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XF_D_J31xx3q"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
        "\n",
        "# Confusion matrix for training data\n",
        "sns.heatmap(confusion_matrix(ytrainOversampled, y_train_pred),\n",
        "            annot=True, fmt=\".0f\", annot_kws={\"size\": 18}, ax=axes[0])\n",
        "axes[0].set_title('Train Data')\n",
        "\n",
        "# Confusion matrix for test data\n",
        "sns.heatmap(confusion_matrix(ytest, y_pred),\n",
        "            annot=True, fmt=\".0f\", annot_kws={\"size\": 18}, ax=axes[1])\n",
        "axes[1].set_title('Test Data')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Zsc9wqxGxx6E"
      },
      "outputs": [],
      "source": [
        "print(\"Train Data\")\n",
        "\n",
        "accuracy_train = accuracy_score(ytrainOversampled, y_train_pred)\n",
        "precision_train = precision_score(ytrainOversampled, y_train_pred, average='binary')\n",
        "recall_train = recall_score(ytrainOversampled, y_train_pred, average='binary')\n",
        "f1_train = f1_score(ytrainOversampled, y_train_pred, average='binary')\n",
        "\n",
        "print(f\"Accuracy: {accuracy_train}\")\n",
        "print(f\"Precision: {precision_train}\")\n",
        "print(f\"Recall: {recall_train}\")\n",
        "print(f\"F1 Score: {f1_train}\")\n",
        "print()\n",
        "print(\"Test Data\")\n",
        "\n",
        "accuracy_test = accuracy_score(ytest, y_pred)\n",
        "precision_test = precision_score(ytest, y_pred, average='binary')\n",
        "recall_test = recall_score(ytest, y_pred, average='binary')\n",
        "f1_test = f1_score(ytest, y_pred, average='binary')\n",
        "\n",
        "print(f\"Accuracy: {accuracy_test}\")\n",
        "print(f\"Precision: {precision_test}\")\n",
        "print(f\"Recall: {recall_test}\")\n",
        "print(f\"F1 Score: {f1_test}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qQw8iuJUxx8c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "K9hwGOCoxx_P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-MxM_HUIxgCq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKngaCd_yQ3x"
      },
      "source": [
        "#### Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yPZeSTtKyRB1"
      },
      "outputs": [],
      "source": [
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [5, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "grid_search_rf = GridSearchCV(estimator=rf_model, param_grid=rf_param_grid, cv=2, scoring='f1', verbose=1, n_jobs=-1)\n",
        "\n",
        "grid_search_rf.fit(XtrainOversampled, ytrainOversampled)\n",
        "\n",
        "# Best parameters\n",
        "best_rf_params = grid_search_rf.best_params_\n",
        "print(\"Best Parameters for RandomForest:\", best_rf_params)\n",
        "\n",
        "# Best model\n",
        "rfc_model = grid_search_rf.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3enS-YOhyREG"
      },
      "outputs": [],
      "source": [
        "# Fit the model on the training set\n",
        "rfc_model.fit(XtrainOversampled, ytrainOversampled)\n",
        "\n",
        "# Make predictions on the training set\n",
        "y_train_pred = rfc_model.predict(XtrainOversampled)\n",
        "\n",
        "# Calculate ROC-AUC score on the training set\n",
        "roc_auc_train = roc_auc_score(ytrainOversampled, rfc_model.predict_proba(XtrainOversampled)[:, 1])\n",
        "\n",
        "# Evaluate model performance on the training set\n",
        "accuracy_train = accuracy_score(ytrainOversampled, y_train_pred)\n",
        "report_train = classification_report(ytrainOversampled, y_train_pred)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rfc_model.predict(Xtest)\n",
        "rfc_predProba = rfc_model.predict_proba(Xtest)[:, 1]\n",
        "\n",
        "# Calculate ROC-AUC score on the test set\n",
        "roc_auc = roc_auc_score(ytest, rfc_model.predict_proba(Xtest)[:, 1])\n",
        "\n",
        "# Evaluate model performance on the test set\n",
        "accuracy = accuracy_score(ytest, y_pred)\n",
        "report = classification_report(ytest, y_pred)\n",
        "\n",
        "# Calculate F1 score on the training set\n",
        "f1_train = f1_score(ytrainOversampled, y_train_pred)\n",
        "\n",
        "# Calculate F1 score on the test set\n",
        "f1_test = f1_score(ytest, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "igT44oKzyRF_"
      },
      "outputs": [],
      "source": [
        "# Print F1 scores\n",
        "print(\"\\nTraining Set F1 Score:\", f1_train)\n",
        "print(\"Test Set F1 Score:\", f1_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MXEdxE9ZyRIV"
      },
      "outputs": [],
      "source": [
        "# Print metrics for both training and test sets\n",
        "print(\"\\nTraining Set Metrics:\")\n",
        "print(f\"Accuracy: {accuracy_train}\")\n",
        "print(f\"ROC-AUC: {roc_auc_train}\")\n",
        "print(\"Classification Report:\\n\", report_train)\n",
        "\n",
        "print(\"\\nTest Set Metrics:\")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"ROC-AUC: {roc_auc}\")\n",
        "print(\"Classification Report:\\n\", report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lQfZsir7yRKt"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
        "\n",
        "# Confusion matrix for training data\n",
        "sns.heatmap(confusion_matrix(ytrainOversampled, y_train_pred),\n",
        "            annot=True, fmt=\".0f\", annot_kws={\"size\": 18}, ax=axes[0])\n",
        "axes[0].set_title('Train Data')\n",
        "\n",
        "# Confusion matrix for test data\n",
        "sns.heatmap(confusion_matrix(ytest, y_pred),\n",
        "            annot=True, fmt=\".0f\", annot_kws={\"size\": 18}, ax=axes[1])\n",
        "axes[1].set_title('Test Data')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QQiCLjXvyRNa"
      },
      "outputs": [],
      "source": [
        "print(\"Train Data\")\n",
        "\n",
        "accuracy_train = accuracy_score(ytrainOversampled, y_train_pred)\n",
        "precision_train = precision_score(ytrainOversampled, y_train_pred, average='binary')\n",
        "recall_train = recall_score(ytrainOversampled, y_train_pred, average='binary')\n",
        "f1_train = f1_score(ytrainOversampled, y_train_pred, average='binary')\n",
        "\n",
        "print(f\"Accuracy: {accuracy_train}\")\n",
        "print(f\"Precision: {precision_train}\")\n",
        "print(f\"Recall: {recall_train}\")\n",
        "print(f\"F1 Score: {f1_train}\")\n",
        "print()\n",
        "print(\"Test Data\")\n",
        "\n",
        "accuracy_test = accuracy_score(ytest, y_pred)\n",
        "precision_test = precision_score(ytest, y_pred, average='binary')\n",
        "recall_test = recall_score(ytest, y_pred, average='binary')\n",
        "f1_test = f1_score(ytest, y_pred, average='binary')\n",
        "\n",
        "print(f\"Accuracy: {accuracy_test}\")\n",
        "print(f\"Precision: {precision_test}\")\n",
        "print(f\"Recall: {recall_test}\")\n",
        "print(f\"F1 Score: {f1_test}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3woWWV4fygCY"
      },
      "source": [
        "#### Model Stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "diK4CyB-yRUE"
      },
      "outputs": [],
      "source": [
        "# model stacking\n",
        "final_predProba = (rfc_predProba + xgb_predProba + ada_predProba)/3\n",
        "\n",
        "# optimise for threshold\n",
        "best_threshold = 0\n",
        "max_score = 0\n",
        "for t in range(30):\n",
        "    threshold = 0.4 + t*0.01\n",
        "    final_pred = [1 if p>threshold else 0 for p in final_predProba]\n",
        "    f1 = f1_score(ytest, final_pred, average='binary')\n",
        "    accuracy = accuracy_score(ytest, final_pred)\n",
        "    score = round(f1*0.8, 4) + round(accuracy*0.2, 4) # adjust weightage round(f1, 4)\n",
        "    if score>max_score:\n",
        "        max_score = score\n",
        "        best_threshold = threshold\n",
        "final_pred = [1 if p>best_threshold else 0 for p in final_predProba]\n",
        "print(f'The best threshold is {best_threshold}')\n",
        "\n",
        "accuracy = accuracy_score(ytest, final_pred)\n",
        "precision = precision_score(ytest, final_pred, average='binary')\n",
        "recall = recall_score(ytest, final_pred, average='binary')\n",
        "f1 = f1_score(ytest, final_pred, average='binary')\n",
        "sns.heatmap(confusion_matrix(ytest, final_pred), annot=True)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqLUxzyOMers"
      },
      "source": [
        "## The cell below is **NOT** to be removed\n",
        "##### The function is to be amended so that it accepts the given input (dataframe) and returns the required output (list).\n",
        "##### It is recommended to test the function out prior to submission\n",
        "-------------------------------------------------------------------------------------------------------------------------------\n",
        "##### The hidden_data parsed into the function below will have the same layout columns wise as the dataset *SENT* to you\n",
        "##### Thus, ensure that steps taken to modify the initial dataset to fit into the model are also carried out in the function below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "snp1IFhkMers"
      },
      "outputs": [],
      "source": [
        "def testing_hidden_data(hidden_data: pd.DataFrame) -> list:\n",
        "    df = hidden_data\n",
        "    # filter out anomylised filters first  or ('n_months_last_bought_' in col) hlthclaim_cnt_success giclaim_cnt_success recency_clmcon_regis affcon_visit_days n_months_since_visit_affcon\n",
        "    anomylisedIdentifiers = [col for col in df.columns if ('ape_' in col) or ('sumins_' in col) or ('prempaid_' in col) or ('flg_hlthclaim_' in col) or ('flg_gi_claim_' in col) or ('f_ever_bought_' in col)\n",
        "                                                                           or ('n_months_last_bought_' in col) or ('lapse_ape_' in col) or ('n_months_since_lapse_' in col) or ('hlthclaim_amt' in col) or ('giclaim_amt' in col) or ('recency_hlthclaim' in col)\n",
        "                                                                           or ('recency_giclaim' in col)\n",
        "                                                                            or ('flg_affconnect_' in col) or ('f_hold' in col)]\n",
        "\n",
        "    insuranceRelated = [col for col in df.columns if ('ape_' in col) or ('sumins_' in col) or ('prempaid_' in col) or ('flg_hlthclaim_' in col) or ('flg_gi_claim_' in col) or ('f_ever_' in col)\n",
        "                    or ('n_months_last_bought_' in col) or ('lapse_ape_' in col) or ('n_months_since_lapse_' in col) or ('hlthclaim_amt' in col) or ('giclaim_amt' in col) or ('recency_hlthclaim' in col)\n",
        "                    or ('recency_giclaim' in col) or ('hlthclaim_cnt_success' in col) or ('giclaim_cnt_success' in col)\n",
        "                    or ('flg_affconnect_' in col) or ('affcon_visit_days' in col) or ('n_months_since_visit_affcon' in col) or ('f_hold' in col)]\n",
        "\n",
        "    # preprocessing for anomylised columns\n",
        "    df['ape_overview'] = df[[col for col in df.columns if ('ape' in col)]].fillna(0).sum(axis=1)\n",
        "    for each in [col for col in df.columns if ('n_months_since_lapse_' in col)]:\n",
        "        df[f'{each}'] = df[f'{each}'].apply(lambda x: 0 if x=='9999' or pd.isna(x) else float(x))\n",
        "    df['nmonths_overview'] = df[[col for col in df.columns if ('n_months_since_lapse_' in col)]].fillna(0).sum(axis=1)\n",
        "    df['prempaid_overview'] = df[[col for col in df.columns if ('prempaid_' in col)]].fillna(0).sum(axis=1)\n",
        "    df['fhold_overview'] = df[[col for col in df.columns if ('f_hold' in col)]].fillna(0).sum(axis=1)\n",
        "    df['fever_overview'] = df[[col for col in df.columns if ('f_ever' in col)]].fillna(0).sum(axis=1)\n",
        "    df['hlthclaim_overview'] = df[[col for col in df.columns if ('recency_hlthclaim' in col)]].fillna(0).sum(axis=1)\n",
        "    df['fig_overview'] = df[[col for col in df.columns if ('flg_affconnect_' in col)]].fillna(0).sum(axis=1)\n",
        "    df['giclaim_overview'] = df[[col for col in df.columns if ('recency_giclaim' in col)]].fillna(0).sum(axis=1)\n",
        "    df['sumins_overview'] = df[[col for col in df.columns if ('sumins_' in col)]].fillna(0).sum(axis=1)\n",
        "\n",
        "    df.prempaid_overview = df.prempaid_overview.astype(int).apply(lambda x: np.log(x+1))\n",
        "    df.ape_overview = df.ape_overview.astype(int).apply(lambda x: np.log(x+1))\n",
        "    df.sumins_overview = df.sumins_overview.astype(int).apply(lambda x: np.log(x+1))\n",
        "    additionalColumns = ['ape_overview', 'nmonths_overview', 'prempaid_overview', 'fhold_overview', 'fever_overview', 'hlthclaim_overview', 'fig_overview', 'giclaim_overview', 'sumins_overview']\n",
        "\n",
        "    pca = PCA(n_components=7)\n",
        "    insuranceDf = pca.fit_transform(df[insuranceRelated].fillna(0))\n",
        "    additionalDf = pd.concat([df[additionalColumns].reset_index().drop('index', axis=1), pd.DataFrame(insuranceDf)], axis=1)\n",
        "\n",
        "    generalInfo = [col for col in df.columns if col not in anomylisedIdentifiers] + ['n_months_last_bought_products']\n",
        "    generalInfo = [col for col in generalInfo if col not in ['recency_cancel', 'tot_cancel_pols', 'clmcon_visit_days', 'recency_clmcon']]\n",
        "    # create a general copy\n",
        "    general = df[generalInfo].reset_index().drop('index', axis=1)\n",
        "    generalCopy = general.copy() # make a copy of the original dataset\n",
        "\n",
        "    # concat with the additionalDf set\n",
        "    general = pd.concat([general.reset_index().drop('index', axis=1), pd.DataFrame(insuranceDf)], axis=1)\n",
        "    # # data cleaning for general columns\n",
        "    # f_purchase_lh - fix purchase (target) as 0 for null value\n",
        "    general.f_purchase_lh = general.f_purchase_lh.fillna(0)\n",
        "\n",
        "    # race - fillna with undisclosed\n",
        "    general.race_desc = general.race_desc.fillna('Undisclosed')\n",
        "\n",
        "    # gender - fillna as undisclosed\n",
        "    general.cltsex_fix = general.cltsex_fix.fillna(\"Undisclosed\")\n",
        "\n",
        "    # n_months_last_bought_products - fillna as\n",
        "    general.n_months_last_bought_products = general.n_months_last_bought_products.fillna(0)\n",
        "\n",
        "    # all flags - fillna as -1 to seperate from the provided values, non submission in flags may be indicative of not purchasin\n",
        "    # all is_ binary attributes - fillna as -1 to seperate from the provided values, non submission in any of the attributes may be indicative of not purchasing\n",
        "    def fillNA(value):\n",
        "        general.flg_substandard = general.flg_substandard.fillna(value)\n",
        "        general.flg_is_borderline_standard = general.flg_is_borderline_standard.fillna(value)\n",
        "        general.flg_is_revised_term = general.flg_is_revised_term.fillna(value)\n",
        "        general.flg_has_life_claim = general.flg_has_life_claim.fillna(value)\n",
        "        general.flg_gi_claim = general.flg_gi_claim.fillna(value)\n",
        "        general.flg_is_proposal = general.flg_is_proposal.fillna(value)\n",
        "        general.flg_with_preauthorisation = general.flg_with_preauthorisation.fillna(value)\n",
        "        general.flg_is_rental_flat = general.flg_is_rental_flat.fillna(value)\n",
        "        general.flg_has_health_claim = general.flg_has_health_claim.fillna(value)\n",
        "        general.flg_is_returned_mail = general.flg_is_returned_mail.fillna(value)\n",
        "\n",
        "        general.is_consent_to_mail = general.is_consent_to_mail.fillna(value)\n",
        "        general.is_consent_to_email = general.is_consent_to_email.fillna(value)\n",
        "        general.is_consent_to_call = general.is_consent_to_call.fillna(value)\n",
        "        general.is_consent_to_sms = general.is_consent_to_sms.fillna(value)\n",
        "        general.is_valid_dm = general.is_valid_dm.fillna(value)\n",
        "        general.is_valid_email = general.is_valid_email.fillna(value)\n",
        "        general.is_housewife_retiree = general.is_housewife_retiree.fillna(value)\n",
        "        general.is_sg_pr = general.is_sg_pr.fillna(value)\n",
        "        general.is_class_1_2 = general.is_class_1_2.fillna(value)\n",
        "        general.is_dependent_in_at_least_1_policy = general.is_dependent_in_at_least_1_policy.fillna(value)\n",
        "    fillNA(-1)\n",
        "\n",
        "    # giclaim_cnt_unsuccess - array of nulls, to be remove\n",
        "    general = general.drop('giclaim_cnt_unsuccess', axis=1)\n",
        "    # hlthclaim_cnt_unsuccess + hlthclaim_cnt_success - transform column into either tried to claim before or not\n",
        "    general.hlthclaim_cnt_unsuccess = general.hlthclaim_cnt_unsuccess.fillna(0)\n",
        "    general.hlthclaim_cnt_success = general.hlthclaim_cnt_success.fillna(0)\n",
        "    general['made_unsuccess_hlthclaim'] = general.hlthclaim_cnt_unsuccess.apply(lambda x: 1 if x!=0 else 0)\n",
        "    general['made_success_hlthclaim'] = general.hlthclaim_cnt_success.apply(lambda x: 1 if x!=0 else 0)\n",
        "    general = general.drop(['hlthclaim_cnt_unsuccess', 'hlthclaim_cnt_success'], axis=1)\n",
        "    general.giclaim_cnt_success = general.giclaim_cnt_success.fillna(0)\n",
        "    general.giclaim_cnt_success = general.giclaim_cnt_success.apply(lambda x: 1 if x!=0 else 0)\n",
        "    general.recency_clmcon_regis = general.recency_clmcon_regis.fillna(0)\n",
        "    general.recency_clmcon_regis = general.recency_clmcon_regis.apply(lambda x: 1 if x!=0 else 0)\n",
        "    general.affcon_visit_days = general.affcon_visit_days.fillna(0)\n",
        "    general.affcon_visit_days = general.affcon_visit_days.apply(lambda x: 1 if x!=0 else 0)\n",
        "    general.n_months_since_visit_affcon = general.n_months_since_visit_affcon.fillna(0)\n",
        "    general.n_months_since_visit_affcon = general.n_months_since_visit_affcon.apply(lambda x: 1 if x!=0 else 0)\n",
        "\n",
        "    # f_ever_declined_la - fill in 0 for null values\n",
        "    general.f_ever_declined_la = general.f_ever_declined_la.fillna(0)\n",
        "\n",
        "    # hh_size_est - keep only hh_size_est, drop hh_20, pop_20, hh_size as hh_size_est is derived from the three, fillna as -1 to differentiate from given value\n",
        "    general.hh_size_est = general.hh_size_est.fillna('-1')\n",
        "    general = general.drop(['hh_20', 'pop_20', 'hh_size'], axis=1)\n",
        "\n",
        "    # annual_income_est -  fillna as -1 to differentiate from given value\n",
        "    general.annual_income_est = general.annual_income_est.fillna(\"F-UNDISCLOSED\")\n",
        "\n",
        "    # recency_lapse - fillna with median\n",
        "    general.recency_lapse = general.recency_lapse.fillna(general.recency_lapse.median())\n",
        "\n",
        "    # drop the clntnum column\n",
        "    general = general.drop('clntnum', axis=1)\n",
        "\n",
        "    # # preprocess text columns - clttype, ctrycode_desc, stat_flag, cltsex_fix, annual_income_est, hh_size_est\n",
        "    # one hot encoding for nominal columns - clttype, ctrycode_desc, cltsex_fix\n",
        "    encoder = OneHotEncoder()\n",
        "    one_hot_encoded = encoder.fit_transform(general[['race_desc', 'clttype', 'ctrycode_desc', 'cltsex_fix']])\n",
        "    one_hot_df = pd.DataFrame(one_hot_encoded.toarray(), columns=encoder.get_feature_names_out(['race_desc', 'clttype', 'ctrycode_desc', 'cltsex_fix']))\n",
        "    general = pd.concat([general, one_hot_df], axis=1)\n",
        "    general = general.drop(['race_desc', 'clttype', 'ctrycode_desc', 'cltsex_fix'], axis=1)\n",
        "\n",
        "    # label encoding for ordinal columns - annual_income_est, stat_flag, hh_size_est\n",
        "    custom_order = ['ACTIVE', 'LAPSED', 'MATURED']\n",
        "    ordinal_encoder = OrdinalEncoder(categories=[custom_order])\n",
        "    general['stat_flag'] = ordinal_encoder.fit_transform(general[['stat_flag']])\n",
        "\n",
        "    custom_order = ['F-UNDISCLOSED', 'E.BELOW30K', 'D.30K-60K', 'C.60K-100K', 'B.100K-200K', 'A.ABOVE200K']\n",
        "    ordinal_encoder = OrdinalEncoder(categories=[custom_order])\n",
        "    general['annual_income_est'] = ordinal_encoder.fit_transform(general[['annual_income_est']])\n",
        "    custom_order = ['-1', '0', '1', '2', '3', '4', '>4']\n",
        "    ordinal_encoder = OrdinalEncoder(categories=[custom_order])\n",
        "    general['hh_size_est'] = ordinal_encoder.fit_transform(general[['hh_size_est']])\n",
        "\n",
        "    # preprocess the datetime columns - retain delta of years for information\n",
        "    general.min_occ_date = general.min_occ_date.apply(lambda x : x[:4])\n",
        "    general.cltdob_fix = general.cltdob_fix.apply(lambda x : x[:4])\n",
        "\n",
        "    # min_occ_date -  replace with median (8.0) if null value observed\n",
        "    general.min_occ_date =  general.min_occ_date.replace('None', pd.NA)\n",
        "    general.min_occ_date = general.min_occ_date.apply(lambda x: 8.0 if pd.isna(x) else 2024 - int(x))\n",
        "\n",
        "    # for cltdob_fix - replace with median (45) if null value observed\n",
        "    general.cltdob_fix =  general.cltdob_fix.replace('None', pd.NA)\n",
        "    general.cltdob_fix = general.cltdob_fix.apply(lambda x: 45 if pd.isna(x) else 2024 - int(x))\n",
        "\n",
        "    rfc_pred_proba = rfc_model.predict_proba(general)[:, 1]\n",
        "    xgb_pred_proba = xgb_model.predict_proba(general)[:, 1]\n",
        "    ada_pred_proba = ada_model.predict_proba(general)[:, 1]\n",
        "\n",
        "    final_pred_proba = (rfc_pred_proba + xgb_pred_proba + ada_pred_proba) / 3\n",
        "\n",
        "    # Determine final predictions using the best_threshold\n",
        "    result = [1 if p > best_threshold else 0 for p in final_pred_proba]\n",
        "\n",
        "    return general"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "crOSdfNl7cEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv95hGPJMers"
      },
      "source": [
        "##### Cell to check testing_hidden_data function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "H5ruLqohMert"
      },
      "outputs": [],
      "source": [
        "# This cell should output a list of predictions.\n",
        "test_df = pd.read_parquet(filepath)\n",
        "test_df = test_df.drop(columns=[\"f_purchase_lh\"])\n",
        "print(testing_hidden_data(test_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWWtx5x1Mert"
      },
      "source": [
        "### Please have the filename renamed and ensure that it can be run with the requirements above being met. All the best!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "aex-EvPJxi_r",
        "62BVoqdLOyeC",
        "sMi_OA92Tjla"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}